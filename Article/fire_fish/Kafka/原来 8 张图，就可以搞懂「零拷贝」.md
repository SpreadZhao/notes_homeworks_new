
参考博客：https://www.cnblogs.com/xiaolincoding/p/13719610.html


## 为什么要有 DMA 技术?
在没有 DMA 技术前，I/O 的过程是这样的：
* CPU 发出对应的指令给磁盘控制器，然后返回；
* 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个中断；
* CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是无法执行其他任务的。
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/I_O%20%E4%B8%AD%E6%96%AD.png)

可以看到，整个数据的传输过程，都要需要 CPU 亲自参与搬运数据的过程，而且这个过程，CPU 是不能做其他事情的。


那么什么是DMA，**直接内存访问（Direct Memory Access） 技术。**

什么是 DMA 技术？简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务。

那使用 DMA 控制器进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/DRM%20I_O%20%E8%BF%87%E7%A8%8B.png)

具体过程：
* 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
* 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
* DMA 进一步将 I/O 请求发送给磁盘；
* 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
* **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；**
* 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
* CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；


可以看到， 整个数据传输的过程，CPU 不再参与数据搬运的工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。


## 传统的文件传输有多么糟糕
传统 I/O 的工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。
代码通常如下，一般会需要两个系统调用
```c++
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
```
代码很简单，虽然就两行代码，但是这里面发生了不少的事情。
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)
执行read系统调用时，一开始cpu操作的是用户空间的内存，为了执行read，必须换到内核空间执行，所以就发生了一次"用户态和内核态的切换"
操作系统执行完read系统调用后，总要把读取到的数据交给应用程序吧，所以就由内核空间到了用户空间，发生了一次"用户态和内核态的切换"
总之就是一次系统调用，发生了2次"用户态和内核态的切换"
**而read、write系统调用共发生了4次"用户态和内核态的切换"，而且发生了4次数据拷贝**
* 第一次拷贝，把磁盘上的数据拷贝到操作系统内核的缓冲区里，这个拷贝的过程是通过 DMA 搬运的。
* 第二次拷贝，把内核缓冲区的数据拷贝到用户的缓冲区里，于是我们应用程序就可以使用这部分数据了，这个拷贝到过程是由 CPU 完成的。
* 第三次拷贝，把刚才拷贝到用户的缓冲区里的数据，再拷贝到内核的 socket 的缓冲区里，这个过程依然还是由 CPU 搬运的。
* 第四次拷贝，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程又是由 DMA 搬运的。

所以，<mark>要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」和「内存拷贝」的次数。</mark>

## 如何优化文件传输的性能？
> 先来看看，如何减少「用户态与内核态的上下文切换」的次数呢？


因为一次系统调用就由2次用户态和内核态的切换，**所以要减少切换就要减少系统调用的次数**

> 再来看看，如何减少「数据拷贝」的次数？

在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，
「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。
因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，**因此用户的缓冲区是没有必要存在的。**


## 如何实现零拷贝？
什么是0拷贝：0拷贝不是没有拷贝，而是在用户空间没有拷贝。

零拷贝技术实现的方式通常有 2 种
* mmap + write
* sendfile

### mmap + write
用 mmap() 替换 read() 系统调用函数

`mmap()` 系统调用函数会直接把内核缓冲区里的数据`「映射」`到用户空间。映射相当于只有一个内核空间了没有用户空间了，应用程序通过地址映射直接操作了内核空间(跟共享的意思一样)
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)
具体过程如下：
* 应用进程调用了 mmap() 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
* 应用进程再调用 write()，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
* 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

我们可以得知，通过使用 mmap() 来代替 read()， 可以减少一次数据拷贝的过程。

但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，
而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。

### sendfile

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 sendfile()，函数形式如下：
```c++
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```
首先，它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。
其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)

是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。【看网卡的支持】

于是，从 Linux 内核 2.4 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， 
sendfile() 系统调用的过程发生了点变化，具体过程如下：
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)
* 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里
* 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

<mark>就是所谓的零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，
也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。</mark>

<mark>零拷贝技术，只需要2次上下文切换和2次数据拷贝，就可以完成文件传输，而且2次数据拷贝和切换都不需要cpu参与，2次都是由DMA搬运
所以总体来看，零拷贝技术可以吧文件传输的性能提高一倍</mark>

## 使用零拷贝技术的项目
kafka
nginx

## PageCache 有什么作用？
PageCache 的优点主要是两个： 
* 缓存最近被访问的数据；
* 预读功能；

这两个做法，将大大提高读写磁盘的性能。
但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能


## 大文件传输用什么方式实现
那针对大文件的传输，我们应该使用什么方式呢？
异步IO的工作方式如下图：
![](https://firefish-dev-images.oss-cn-hangzhou.aliyuncs.com/dev-images/%E5%BC%82%E6%AD%A5%20IO%20%E7%9A%84%E8%BF%87%E7%A8%8B.png)
**大文件拷贝不经过"内核空间内存"直接到"用户空间内存"**。其实就是不会使用内核空间的PageCache。
它把读操作分为两部分：
* 前半部分，内核向磁盘发起读请求，但是可以**不等待数据就位就可以返回**，于是进程此时可以处理其他任务；
* 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据

而且，我们可以发现，异步 I/O 并没有涉及到 PageCache，所以使用异步 I/O 就意味着要绕开 PageCache。

**绕开 PageCache 的 I/O 叫直接 I/O**，使用 PageCache 的 I/O 则叫缓存 I/O。通常，对于磁盘，异步 I/O 只支持直接 I/O。

所以：大文件传输使用的是【异步IO + 直接IO】


## 总结
早期 I/O 操作，内存与磁盘的数据传输的工作都是由 CPU 完成的，而此时 CPU 不能执行其他任务，会特别浪费 CPU 资源。

于是，为了解决这一问题，DMA 技术就出现了，每个 I/O 设备都有自己的 DMA 控制器，通过这个 DMA 控制器，CPU 只需要告诉 DMA 控制器，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMA 控制器来完成，CPU 不需要参与数据传输的工作。

传统 IO 的工作方式，从硬盘读取数据，然后再通过网卡向外发送，我们需要进行 4 上下文切换，和 4 次数据拷贝，其中 2 次数据拷贝发生在内存里的缓冲区和对应的硬件设备之间，这个是由 DMA 完成，另外 2 次则发生在内核态和用户态之间，这个数据搬移工作是由 CPU 完成的。

为了提高文件传输的性能，于是就出现了零拷贝技术，它通过一次系统调用（sendfile 方法）合并了磁盘读取与网络发送两个操作，降低了上下文切换次数。另外，拷贝数据都是发生在内核中的，天然就降低了数据拷贝的次数。

afka 和 Nginx 都有实现零拷贝技术，这将大大提高文件传输的性能。

零拷贝技术是基于 PageCache 的，PageCache 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 I/O 调度算法实现了 IO 合并与预读，这也是顺序读比随机读性能好的原因。这些优势，进一步提升了零拷贝的性能。

需要注意的是，零拷贝技术是不允许进程对文件内容作进一步的加工的，比如压缩数据再发送

另外，当传输大文件时，不能使用零拷贝，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，并且大文件的缓存命中率不高，这时就需要使用「异步 IO + 直接 IO 」的方式。

在 Nginx 里，可以通过配置，设定一个文件大小阈值，针对大文件使用异步 IO 和直接 IO，而对小文件使用零拷贝。




