# 1. 前言

本次上机主要是为了熟悉hadoop的分布式文件系统的结构，以及配置一个完全的分布式系统。在操作的过程中确实遇到了很多问题，比如ssh协议对于文件夹写权限的控制、linux各个用户的文件夹是分开的等等。其中最多的其实是无数的`Permission Denied`问题，这也体现了分布式系统中安全的必要性。总之，经过8个小时左右的不断摸索，最终终于是成功地搞定了这项看起来比较宏大的工程。

**另外报告中的图片看起来有些模糊，是因为截图取自我做实验的==全过程录屏==，使用的分辨率比较低。**

# 2. 实验名称

第一次实验：Hadoop安装与部署

# 3. 实验日期

2022年9月28日    实验室G340

# 4. 实验学生

20009200303 赵传博

# 5. 实验内容

* 配置Hadoop的分布式环境，可以先配置伪分布式环境以熟悉流程
* 在Hadoop上学习使用HDFS

# 6. 实验思路、结构

## 6.1 准备工作

在完全分布式的配置中，需要三台虚拟机，并且每台虚拟机上需要有Hadoop的本体，以及老师建议我们安装的Zookeeper，另外这些软件都是基于jvm的，所以我们还需要安装jdk。

## 6.2 Master的"出生"

在集群环境中，每一个结点都需要时时刻刻与其他结点通信，所以DHCP服务器为我们的虚拟机分配的**动态IP**肯定是==不合适==的。因此我们需要先把我们的Master改为**静态IP**。

### 6.2.1 Virtual Network Editor

打开`Edit -> VritualNetwork Editor`，之后会看到如下界面：

![[Pasted image 20220927174547.png]]

我们首先需要获得管理员权限来修改网络的设置，点击`Change Settings`。之后就能够修改其中的内容了。

之后将`Use local DHCP service to distribute IP address to VMs`前面的选项卡取消掉，点击`Apply`即可。

![[Pasted image 20220927174643.png]]

然后点击`NAT Settings`来查看网关(Gateway)，其中的网关IP就是我们需要记住的地址：

![[Pasted image 20220927175011.png]]

### 6.2.2 Network Plan

接下来就是真正配置静态IP的过程了。老师给的例子中使用的是CentOS，而我使用的是Ubuntu，所以自己在网上查到了在Ubuntu中配置静态IP的方法。

首先使用如下命令来查看自己的网卡设备：

```shell
nmcli connection show
```

然后就能够看到如下输出：

![[Pasted image 20220927175420.png]]

其中的`DEVICE`一项的内容就是我们之后要用到的。Ubuntu的网络配置文件位于`/etc/netplan`中，在我的机器上叫做`01-network-manager-all.yaml`，打开它，并做如下修改：

```yaml
network:
	ethernets:
		ens33:                 # 我们的网卡设备
			dhcp4: false       # 禁用dhcp
		address:
			- 192.168.217.44/24   # 要改的静态ip，最后一个44可以任意，/24代表子网掩码
		routes:
			- to: default
			via: 192.168.217.2  # Gateway
		nameservers:
		    addresses: [8.8.8.8,114.114.114.114]    # 内网和外网的DNS
    version: 2
```

^3af7e7

然后开始应用这个修改方案。在应用之前，首先要打开`networkd`服务：

```shell
sudo systemctl start systemd-networkd
```

也可以检测一下究竟是否运行成功：

```shell
sudo systemctl status systemd-networkd
```

![[Pasted image 20220927181339.png]]

> active表示正在运行

最后就可以应用这条个方案了：

```shell
sudo netplan apply
```

最后可以使用`ifconfig`来检查电脑的网络配置，如果IP显示的就是我们刚才更改的静态IP，说明我们成功了！

修改静态ip结束之后，我试着ping了一下常用的网址。发现很多都ping不到，即使是localhost都不行。后来才在Stack OverFlow上找到了原因：其实是因为我使用的IP的最后一位和其他的进程有冲突，所以才ping不到。换几个ip试试就可以了。

### 6.2.3 Hostname

有了IP，还要有域名，这样在通信的时候才更方便。很显然，我们的域名就要叫`master`。使用`hostname`来查看本机的域名；使用`hostnamectl set-hostname master`来修改原来的域名为`master`。

## 6.3 Slave的"出生"

有了master，接下来就要开始创建我们的从结点了。这里直接在master的基础上进行**完全克隆**，具体的操作就不展示了，这里只贴出来最终的效果：

![[Pasted image 20220927181847.png]]

> Ubuntu-Ha是master，Ubuntu-Ha-S1和Ubuntu-Ha-S2是两个slave。

克隆好的这两个虚拟机，目前的静态IP和Hostname和master都是一样的，所以我们要分别对这两个仆从进行修改。

具体的修改很简单，将IP更改为其他的任意值(==但是需要在同一Gateway下==)，再将Hostname修改为`slave1`和`slave2`。这样最基本的工作就算是完成了。

## 6.4 结点之间的通信

有了master和两个slave，现在他们还相互不认识，那么怎么让它们能够记住互相的名字呢？在`/etc/hosts`处登记一下就可以了：

![[Pasted image 20220927182630.png]]

需要注意的是：在三台虚拟机上都要登记，别只登记了一台哦！

## 6.5 配置ssh

其中最重要的就是本机生成的**公钥(public key)**和在本机中的认证文件`~/.ssh/authorized_keys`。原理就是：公钥文件就是一坨字符串，将A的公钥文件拷贝到B的`authorized_keys`中就能够实现<u>在A使用ssh访问B的时候不需要密码认证。</u>也就相当于是A拿着一个钥匙，而B认可这个钥匙，所以A能打开B。

通过以上解释，我们不难看出，本实验需要做的就是让这三个结点在互相访问的时候都不需要密码，所以我们需要在三台机器上分别生成他们的公钥：

```shell
ssh-keygen -t rsa
```

这样我们就得到了在三个机器上的三个公钥文件。然后就是将这三个文件添加到三个机器上的三个`authorized_keys`中，做到**每个人都认可我们仨。**

复制的方法有很多种，可以使用`scp`命令，也可以使用`ssh-copy-id`命令，总之在操作完成之后，我们在接下来的过程中几乎不会遇到`Permission denied`这种问题。而不幸的是，我本人遇到了很多很多次，所以这里的配置真的很重要。

## 6.6 关闭防火墙

因为我们进行的实验比较初级，所以关闭防火墙能解决掉很多麻烦。

很巧的是，我下的虚拟机根本没有安装防火墙，所以也就不存在关闭的问题了。

## 6.7 时间同步

老实说，我并不知道时间同步和hadoop的配置有什么关系，我怀疑可能是之后的**分布式文件系统需要保证文件的一致性**，所以这里简要介绍一下即可。

首先在三台机器上都安装上ntp：

```shell
sudo apt install ntp
```

然后设置开启ntp服务：

```shell
systemctl enable ntp && systemctl start ntp
```

**CentOS中ntp服务叫做ntpd；而Ubuntu中叫做ntp**

之后也可以检测一下服务是否运行成功：

```shell
systemctl status ntp
```

## 6.8 准备安装目录

接下来就正式进入到和hadoop和zookeeper相关的内容了。首先自然是准备所有的安装目录，就像我们经常会把Windows程序安装到非C盘的`Program Files\`文件夹里一样。

和教程做的一样，我们将所有和实验相关的都放到`/home/work/`目录下。这里要强调一个非常重要的点：**Linux的根目录下有home/目录，而这个目录里的文件夹是各个用户的名字。也就是说，我们的work文件夹和各个用户的文件夹是同级的！**然后在这个文件夹中新建四个文件夹，用途的话看名字就很清楚了。 ^6226d7

在一台虚拟机中执行：

```shell
mkdir /home/work
cd work
mkdir {_src,_app,_logs,_data}
```
> 这里使用大括号可以一次创建多个文件夹

然后将我们的hadoop和zookeeper的源码和发行版本直接copy到对应的目录即可，在接下来的实验中，仔细观察目录就能知道存放的位置了。

至于为什么只在一台虚拟机中执行，是为了接下来对于`scp`命令的练习。这样就能够很方便地远程通过ssh进行数据拷贝。

## 6.9 jdk的安装

在6.1就说过，我们需要安装jdk，所以也一定提前下载好了jdk的安装包。我进行实验的安装包是在openlogic网站下载的8u342版本。

首先对下载好的`.tar.gz`文件进行解压，对于带`.gz`的，使用`xzvf`参数：

```shell
tar xzvf openlogic-openjdk-8u342-b07-linux-x64.tar.gz
```

然后将解压出来的文件夹移动到`/opt/`目录中，这个目录默认是空的，存放的就是用户自己选择(optional)的软件：

```shell
sudo mv openlogic-openjdk-8u342-b07-linux-x64 /opt/
```

接下来开始使用`alternatives`来安装。看起来很高大上，其实原理很简单。我们的所有可执行文件都是在`/opt/openlogic-openjdk-8u342-b07-linux-x64/`中的，那么如何让它看起来变成安装的了呢？使用如下命令：

```shell
sudo update-alternatives --install /usr/bin/java java /opt/openlogic-openjdk-8u342-b07-linux-x64/bin/java 2
```

这段命令其实就是创建了一个软连接，它叫做`java`，保存在`/usr/bin/`目录中，引用的源文件是`/opt/openlogic-openjdk-8u342-b07-linux-x64/bin/java`，优先级是2。而我们知道：`/usr/bin/`目录在系统默认的环境变量中，所以这样操作之后，就让它看起来像被安装了一样。

如果虚拟机中已经安装了多个版本的java，那么这个java软连接可能会指向多个版本的java。那么如何去选择呢？使用下面的命令：

```shell
update-alternatives --config java

There are 2 programs which provide 'java'.

Selection  Command

-----------------------------------------------

1     /opt/install/jdk1.8.0_91/bin/java

2     /opt/install/jdk1.8.0_111/bin/java

Enter to keep the current selection[+], or type selection number: 1

java version "1.8.0_91"

Java(TM) SE Runtime Environment (build 1.8.0_91-b14)

Java HotSpot(TM) 64-Bit Server VM (build 25.91-b14, mixed mode)
```

除了使用`--config`来配置，也可以使用`--set`来让目录修改立即生效。接下来就用这种方法来安装jar和javac：

```shell
# jar
sudo update-alternatives --install /usr/bin/jar jar /opt/openlogic-openjdk-8u342-b07-linux-x64/bin/jar 2

sudo update-alternatives --set jar /opt/openlogic-openjdk-8u342-b07-linux-x64/bin/jar

# javac
sudo update-alternatives --install /usr/bin/javac javac /opt/openlogic-openjdk-8u342-b07-linux-x64/bin/javac 2

sudo update-alternatives --set javac /opt/openlogic-openjdk-8u342-b07-linux-x64/bin/javac
```

**注意：整个6.9的操作在三台虚拟机上都要执行一遍。**

## 6.10 Zookeeper设置

接下来开始逐个软件来进行调试，首先从zookeeper开始。

### 6.10.1 zoo.cfg

首先编写zookeeper的配置文件。这个文件需要自己新建，是`/home/work/_app/zookeeper-3.8.0/conf/zoo.cfg`：

```yaml
# ZooKeeper使用的基本时间单位（以毫秒为单位）。它用于做心跳，最小会话超时将是tickTime的两倍。
tickTime=200

# 存储内存数据库快照的位置，除非另有说明，否则指向数据库更新的事务日志。
dataDir=/home/work/_data/zookeeper-3.8.0

# 用于事务日志的不同目录。
dataLogDir=/home/work/_logs/zookeeper-3.8.0

# 侦听客户端连接的端口
clientPort=2181

# 表示在leader选举结束后，followers与leader同步需要的时间，如果followers比较多或者说leader的数据灰常多时，同步时间相应可能会增加，那么这个值也需要相应增加。当然，这个值也是follower和observer在开始同步leader的数据时的最大等待时间(setSoTimeout)
initLimit=5

# 表示follower和observer与leader交互时的最大等待时间，只不过是在与leader同步完毕之后，进入正常请求转发或ping等消息交互时的超时时间。
syncLimit=2

# server.serverid=host:tickpot:electionport
# server：固定写法
# serverid：每个服务器的指定ID（必须处于1-255之间，必须每一台机器不能重复）
# host：主机名
# tickpot：心跳通信端口
# electionport：选举端口
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```

> 这里的隐患就是：如果之前的work文件夹是在linux自带的那个文件管理器的home/目录下，那么实际上它的存放目录是`/home/<userName>/work`。[[#^6226d7|跳转]]

### 6.10.2 将zoo.cfg发送到三个结点

这三个结点的`zoo.cfg`的配置都是一样的，所以我们直接将`/home/work/`目录整个复制到另外两台虚拟机上是最方便的。那么这里就用到了`scp`命令。这里回顾一下6.5中配置ssh的时候，我们知道公钥和认可都是放在`~/.ssh/`目录下的。那么这就证明了一点：**一个用户对应一个`ssh`，而不是一个机器对应一个`ssh`。**因此我们所有的登陆、访问、传输文件之类的乱七八糟的操作都是在当前用户的基础上的。而我当前登陆的用户在三台虚拟机上都是`spreadzhao`，所以这里只是针对`spreadzhao@<hostName>`来进行操作。网上可能会有各种教程，很多都是直接用的`root`用户，这里需要区分一下在使用shell的时候到底是哪个用户在登陆。

比如要将master的work目录复制到slave1中，就可以这样：

```shell
sudo scp -r /home/work spreadzhao@slave1:/home
```

> -r表示递归复制文件夹，后面分别是源文件和目标文件，目标文件由\<userName\>@\<hostName\>:\<remotePath\>构成。

如果出现`Permission Denied`问题，是因为目标文件的写权限没有开放给\<username\>这个用户。比如spreadzhao怎么能打开根目录的home呢？他只有权利打开`/home/spreadzhao/`以及里面的目录。因此我们要将`/home`文件夹的读写权限修改一下。在slave中执行如下命令：

```shell
sudo chmod 777 /home
```

然后就可以正常进行`scp`复制了。**但是这样做会留下一个隐患，具体的隐患等到后面便会暴露**。 ^932531

### 6.10.3 竞选猴王

和hadoop一样，zookeeper也是一群猴子，而它们之中就有一个需要作为王。谁能当王？是通过**选举**的方式来决定的。

首先，每个猴子需要有一个自己的编号，叫做`myid`。那么这个id在哪里呢？就在我们6.10.1配置的`dataDir`目录下。在三台虚拟机上分别执行：

```shell
# master
echo 1 > /home/work/_data/zookeeper-3.8.0/myid

# slave1
echo 2 > /home/work/_data/zookeeper-3.8.0/myid

#slave2
echo 3 > /home/work/_data/zookeeper-3.8.0/myid
```

## 6.11 设置环境变量

目前为止，我们已经差不多地安装了jdk, zookeeper和hadoop。其中hadoop我们只是复制了文件，还没有进行任何修改。而接下来就可以开始配置环境变量，让程序使用更加方便。这里我们选用的方式是**批处理任务**。这种方式的缺点是**只在当前终端可用**，因此我们只能一直开着这个终端了，不过复用的命令也很简单。

首先定义各种HOME，然后再配置Path，最后应用即可：

```shell
# Hadoop 3.3.4
echo "export HADOOP_HOME=/home/work/_app/hadoop-3.3.4" >> /etc/bashrc
echo "export HADOOP_LOG_DIR=/home/work/_logs/hadoop-3.3.4" >> /etc/bashrc
echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> /etc/bashrc
echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> /etc/bashrc
echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> /etc/bashrc
echo "export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop" >> /etc/bashrc

# Zookeeper 3.8.0
echo "export ZOOKEEPER_HOME=/home/work/_app/zookeeper-3.8.0" >> /etc/bashrc

# JAVA
echo "export JAVA_HOME=/opt/openlogic-openjdk-8u342-b07-linux-x64" >> /etc/bashrc
echo "export JRE_HOME=/opt/openlogic-openjdk-8u342-b07-linux-x64/jre" >> /etc/bashrc

# Path
echo "export PATH=\$PATH:\$JAVA_HOME/bin:\$JRE_HOME/bin:\$HADOOP_HOME/bin:\$HADOOP_HOME/sbin:\$ZOOKEEPER_HOME/bin:\$HBASE_HOME/bin" >> /etc/bashrc

# Apply
source /etc/bashrc

```

执行的时候可能也会遇到`Permission Denied`问题。解决方法是：先用`sudo su / -i`切换成超级用户，先执行一条任意命令创建出`/etc/bashrc`文件，然后将这个文件改写成`777`。这样即使之后不是用`sudo`也能够修改其中的内容了。

## 6.12 开启Zookeeper

一切就绪后，终于可以执行第一条和程序相关的命令了：

```shell
zkServer.sh start
```

能看到如下输出：

![[Pasted image 20220927211712.png]]

在三台虚拟机上都执行上述命令，然后再都执行如下命令：

```shell
jps
```

这个就是jdk中的命令了，通过它就能看到当前结点的状态：

![[Pasted image 20220927212024.png]]

然后我们可以查看一下这些结点的状态，执行如下命令：

```shell
zkServer.sh status
```

可以看到当前结点是leader还是follower。其中的leader就是所谓的"猴王"。在我的实验中，slave1是leader，另外两个是follower。这好像与事实不太相符，原因就是zookeeper的选举机制。具体的过程可以参考[这篇文章](https://baijiahao.baidu.com/s?id=1685254558927619982&wfr=spider&for=pc)。

然后可以检测一下是否连接成功：

```shell
zkCli.sh
```

部分结果：

![[Pasted image 20220927212736.png]]

## 6.13 Hadoop设置

接下来是本实验中最难最繁琐的一部分，我参考了外网的一篇比较不错的文章：

[How to Install and Set Up a 3-Node Hadoop Cluster | Linode](https://www.linode.com/docs/guides/how-to-install-and-set-up-hadoop-cluster/)

我们知道hadoop的最根本就是nameNode，那么我们就从它开始来进行设置，在`/home/work/_app/hadoop-3.3.4/etc/hadoop`目录下(**以下记为\<hadoopetc\>**)修改`core-site.xml`文件：

```xml
<configuration>
	<property>
		<name>fs.default.name</name>
		<value>hdfs://master:9000</value>
	</property>

	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/work/_data/hadoop-3.3.4</value>
	</property>
</configuration>
```

这样我们就设置好了nameNode结点的在文件系统中对应的对象：master。

然后开始设置文件系统HDFS的配置，修改`<hadoopetc>/hdfs-site.xml`文件：

```xml
<configuration>

	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file://${hadoop.tmp.dir}/nameNode</value>
	</property>
	
	<property>
		<name>dfs.datanode.data.dir</name>
		<value>file://${hadoop.tmp.dir}/dataNode</value>
	</property>
	
	<property>
		<name>dfs.replication</name>
		<value>1</value>
	</property>
	
</configuration>
```

然后是任务管理，这部分当然要交给hadoop默认的yarn来管理。修改`<hadoopetc>/mapred-site.xml`文件：

```xml
<configuration>
    <property>
            <name>mapreduce.framework.name</name>
            <value>yarn</value>
    </property>
    <property>
            <name>yarn.app.mapreduce.am.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.map.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
            <name>mapreduce.reduce.env</name>
            <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
</configuration>
```

然后是yarn内部的具体设置。修改`<hadoopetc>/yarn-site.xml`文件：

```xml
<configuration>
    <property>
            <name>yarn.acl.enable</name>
            <value>0</value>
    </property>

    <property>
            <name>yarn.resourcemanager.hostname</name>
            <!-- 这里应该是master结点的静态ip地址 -->
            <value>192.168.217.44</value>
    </property>

    <property>
            <name>yarn.nodemanager.aux-services</name>
            <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

接下来还需要一件事：master为什么是master？因为两个slave在给他打工。这部分我们就要到`<hadoopetc>/workers`中设置：

```
slave1
slave2
```

对于yarn和mapred，需要额外指定一些配置，也就是各种存储空间的大小之类的：

```xml
<!-- yarn-site.xml -->
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
</property>

<property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
</property>

<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
</property>

<!-- mapred-site.xml -->
<property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>512</value>
</property>

<property>
        <name>mapreduce.map.memory.mb</name>
        <value>256</value>
</property>

<property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>256</value>
</property>
```

最后，需要将所有的配置发送到三个结点上。这里使用shell中的for循环来玩儿个花的：

```shell
for slave in slave1 slave2; do scp -r hadoop-3.3.4 spreadzhao@$slave:/home/work/_app/; done
```

> 这里我在实验的时候slave1需要了密码，而slave2不需要。证明我当时的ssh配置是有问题的。

^72d633

## 6.14 使用Hadoop

现在，我们已经有了一个坐落在3台虚拟机上的文件系统了。终于到了使用它的时候！首先，我们需要对整个文件系统来进行格式化：

```shell
hdfs namenode -format
```

> 这里目前是没有问题的，但是在我进行实验的时候却出现了一个小插曲。原因就是，我在配置`core-site.xml`的时候在其中没有设置`hadoop.tmp.dir`，而我之后在配置hdfs的时候却使用了这个变量，导致了我的namenode结点被分配到了一个奇怪的位置。**这令我感觉很不爽！**所以我打算添加完变量之后重新进行一次格式化。不过没想到，就是这次格式化又带来一个麻烦。简单的总结可以是下面这篇文章的第二个格式化问题: [(28条消息) 集群安装中遇到的问题_编程有了模型的博客-CSDN博客](https://blog.csdn.net/qq_1018944104/article/details/85062918)。我在清楚了所有的version之后终于能重新加载dataNode了。其中有一个重要的点是，**在修改完`core-site.xml`之后，别忘了，用各种方式都可以，把它拷贝到slave上**！不然配置不一样还是会报错！

**然后就是本次实验中最大的一个坑**，也就是之前提到过的[[#^932531|隐患]]。使用`start-dfs.sh`命令来启动文件系统，但是却出现了这样的错误：

![[Pasted image 20220927220625.png]]

首先，该脚本位于`/home/work/_app/hadoop3.3.4/sbin/`目录下，在其中第一行注释`#!/usr/bin/env bash`下面要添加一些信息：

```shell
HDFS_DATANODE_USER=spreadzhao
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_ZKFC_USER=spreadzhao
HDFS_JOURNALNODE_USER=spreadzhao
HDFS_NAMENODE_USER=spreadzhao
HDFS_SECONDARYNAMENODE_USER=spreadzhao
```

> 本来这里面是root而不是spreadzhao，而我的实验都是在脱离root用户的环境下做的，所以这里就这样写了。

然后，我又进行了ssh配置那一套，什么拷贝公钥，什么`scp-copy-id`等等，都做了，都没用！依然是原来的错误。直到我看到了Stack OverFlow上的一篇文章，和我的问题很相似，而这个答案看起来很搞笑：**因为我们使用的就不是ssh，而是rsh！**我们使用下面的命令来看一下：

```shell
pdsh -q -w localhost
```

能看到如下结果：

![[Pasted image 20220927221255.png]]

可以看到，Rcmd type一项就是rsh，所以我们把它改成ssh就可以了。

```shell
export PDSH_RCMD_TYPE=ssh
```

另外，我们还需要指定hadoop的JAVA_HOME。修改`<hadoopetc>/hadoop-env.sh`文件，找到`export JAVA_HOME`那一行，在后面添加上我们的java目录：

![[Pasted image 20220927221805.png]]

到这里可能还会出现这样的问题：

![[Pasted image 20220927222033.png]]

在登陆本机和slave2的时候都没有问题，但是在slave1上却出问题了。似乎我们[[#^72d633|之前]]也有这么一个类似的情况，而这就是我们之前的操作带来的[[#^932531|隐患]]。

从一篇CSDN中的文章，我终于知道了原来权限放得太开有时候也会有问题：

 ![[Pasted image 20220927222351.png]]

就是因为我之前瞎操作的过程中，把`/home/spreadzhao/`目录给改成了`777`，导致认可文件根本不会让别人去登陆。所以我们按要求把能改的权限都改一改，终于成功了！

# 7. 实验结果

首先，在启动Hadoop文件系统时，能看到namenode、datanode和secondary namenode都已经正常启动：

![[Pasted image 20220928111959.png]]

然后运行`jps`，能在master上看到如下结果：

![[Pasted image 20220928112223.png]]

能在两个slave上看到如下结果：

![[Pasted image 20220928112247.png]]

这证明我们的集群已经启动成功了！另外我们也可以用Hadoop提供的网页版管理软件来更方便地管理我们的文件系统。在浏览器中输入`http://master:9870`即可。具体的信息可以参考下面这篇文章：

[(28条消息) 远程接入HDFS(9870端口)官方自带可视化管理界面的方法和局限性_ka2x的博客-CSDN博客_9870端口](https://blog.csdn.net/GDUT_xin/article/details/117377364)

然后，还要开启我们的管理器才行！输入`start-yarn.sh`来开启：

![[Pasted image 20220928112936.png]]

能看到，yarn的运行状态也是正常的。接下来我们就可以开始测试文件系统和一个最基础但是也很重要的功能——MapReduce了。首先从测试系统开始，我们需要实现准备好几个txt文本用来测试，我这里选了三个。分别是`book1.txt`、`book2.txt`和`book3.txt`。然后我们要在文件系统中创建它们的存放目录：

```shell
hdfs dfs -mkdir -p /user/hadoop/books
```

然后就可以将这三个文件拷贝到文件系统中了：

```shell
hdfs dfs -put book1.txt book2.txt book3.txt /user/hadoop/books
```

就像本地的电脑一样，我们也可以用`ls`命令去查看它们：

```shell
hdfs dfs -ls /user/hadoop/books
```

然后就能看到如下结果：

![[Pasted image 20220928175234.png]]

我们还可以使用`cat`命令来查看其中的内容：

```shell
hdfs dfs -cat /user/hadoop/books/book2.txt
```

![[Pasted image 20220928175321.png]]

> 这段话是我在网上随便复制的，没啥意义。

然后就可以测试MapReduce了。这里用到了Hadoop提供给我们的一个测试的小工具，在我的电脑上位于`/home/work/_app/hadoop3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar`。这个工具的介绍和功能可以参考下面的文章：

[(28条消息) Hadoop之hadoop-mapreduce-examples测试执行及报错处理_恒悦sunsite的博客-CSDN博客_hadoop-mapreduce-examples](https://blog.csdn.net/carefree2005/article/details/121834803)

在本实验中，我只用到了其中的wordcount功能，用来统计我刚刚拷贝进去的txt中单词的数量。现在就开始！执行如下命令：

```shell
yarn jar /home/work/_app/hadoop3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount "/user/hadoop/books/*" /user/hadoop/output
```

> wordcount后面的两个参数中，第一个是要统计的文本，我这里就是`books/`目录下所有的文件；第二个参数就是保存输出结果的文件夹，这样所有的结果就会保存在`output/`目录下。

完成后，我们可以使用`ls`命令来查看是否成功：

```shell
hdfs dfs -ls /uesr/hadoop/output
```

如果看到如下的`_SUCCESS`，就代表成功了：

![[Pasted image 20220928175953.png]]

最后就是查看我们统计出来的结果了：

```shell
hdfs dfs -cat /user/hadoop/output/part-r-00000 | less
```

以下是我的部分结果：

![[Pasted image 20220928180113.png]]

# 8. 总结建议

虽然本次实验波波折折，捅咕捅咕这个又捅咕捅咕那个，最后也算是有了点效果。但是这确实不算什么。对于**非关系型数据库**的探索才刚刚开始，甚至是没有开始。不过对于Hadoop的这种集群的分布式系统，去了解一下还是很有必要的，毕竟这也是大名鼎鼎的Apache公司打造的。所以在以后的"码农生活中"，还是需要让我们的生活变得不那么单调一些。

**另外，本次实验中遇到的种种问题都已经内涵在了6.x中**，这样对于从头到尾的阅读还是很方便的。emm，也没什么要多说的了，就这样吧，下次hbase要加油！